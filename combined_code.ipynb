{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import numpy as np\n","pixel_mean = np.array([123.675, 116.280, 103.530]).reshape((1, 1, 3))\n","pixel_std = np.array([58.395, 57.120, 57.375]).reshape((1, 1, 3))\n","max_depth_range = 2.1\n","n_planes_for_train = 5\n","n_planes_for_val = 20\n","n_bins_for_plane_hrchy_sampling = 20\n","mesh_data_root = 'demodata'\n","data_h = 512\n","data_w = 512\n","crop_expand_ratio = 0.1\n","binvoxPathPrefix = 'demodata'\n","use_adaptive_sampling = False\n","bin_sample_replace = False\n","depth_range_expand_ratio = 0.1\n","use_masked_out_img = False\n","extra_mesh_data_root = None"],"metadata":{"id":"OECufHz0d2dX","executionInfo":{"status":"ok","timestamp":1670184659324,"user_tz":360,"elapsed":20,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pzRpT3a7pTFT","executionInfo":{"status":"ok","timestamp":1670184660793,"user_tz":360,"elapsed":1487,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}},"outputId":"41142be7-4efe-4c93-df35-2b8118873afe"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/MyDrive/ECE 544 Project/VincentsWorkspace\")\n","import sys\n","sys.path.append('.')\n","%autosave 60"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"72eP2OspPh6e","executionInfo":{"status":"ok","timestamp":1670184660794,"user_tz":360,"elapsed":7,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}},"outputId":"e8b6f0b5-e3b9-4380-9078-ef10cec3f32b"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"application/javascript":["IPython.notebook.set_autosave_interval(60000)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Autosaving every 60 seconds\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","import gc\n","import glob\n","import multiprocessing\n","import trimesh\n","import traceback\n","import skimage\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# import meshplot as mp\n","from tqdm import tqdm\n","from PIL import Image\n","from scipy.ndimage.morphology import binary_erosion, distance_transform_edt\n","from scipy.ndimage import maximum_filter, gaussian_filter\n","from scipy import ndimage\n","from skimage import filters, transform  \n","import binvox_rw\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","\n","class MeshObj:\n","    def __init__(self,fn, max_depth_range=None, fnroot=\"\", binvoxPathPrefix='', extra_fnroot=None, pure_infer=False):\n","        self.float_type = np.float32\n","        arr = fn.split('/')\n","        self.csName = arr[-4]\n","        self.objID = int(arr[-3].split('_')[0])\n","        self.frameID = arr[-2].split('_')[0]\n","        self.IMAGE_SIZE = (800, 1280)\n","        self.fnroot = fnroot\n","        self.fn = self.fnroot + fn[1:]\n","\n","        self.extra_fnroot = extra_fnroot\n","        if self.extra_fnroot is not None:\n","            self.extra_fn = self.extra_fnroot + fn[1:]\n","        else:\n","            self.extra_fn = None\n","\n","        self.imfn = self.fnroot + '/{}/images/{}.bmp'.format(self.csName,self.frameID)\n","        if self.extra_fnroot is not None:\n","            self.depthfn = self.extra_fnroot + '/{}/depth/{}.npy'.format(self.csName,self.frameID)\n","            self.visfn = self.extra_fnroot + '/{}/visible/{}.npy'.format(self.csName,self.frameID)\n","        else:\n","            self.depthfn = self.fnroot + '/{}/depth/{}.npy'.format(self.csName,self.frameID)\n","            self.visfn = self.fnroot + '/{}/visible/{}.npy'.format(self.csName,self.frameID)\n","        \n","        self.pure_infer = pure_infer\n","        if not self.pure_infer:\n","            self.binvox = binvoxPathPrefix + self.GetObjFN()[1:-8] + 'voxel.binvox2'\n","            if not os.path.exists(self.binvox):\n","                self.binvox = self.binvox[:-1]\n","            if not os.path.exists(self.binvox):\n","                self.binvox = os.path.join(os.path.dirname(self.binvox), \"voxel_256.binvox2\")\n","            assert os.path.exists(self.binvox), f\"{self.binvox}\"\n","\n","        self.LoadRMatrices()\n","\n","        self.avoid_nan_eps = 1e-8\n","    \n","    def compute_visible_mesh_depth_range(self):\n","        xx, yy, xxi, yyi, select, obj = self.ProjectObjToImage()\n","        vis_verts = np.array(obj.vertices)[select, :]\n","        min_coords = np.min(vis_verts, axis=0)\n","        max_coords = np.max(vis_verts, axis=0)\n","        depth_range = max_coords[2] - min_coords[2]\n","        return depth_range, max_coords[2]\n","        \n","    def LoadRMatrices(self):\n","        if self.extra_fn is not None:\n","            rmatrices_file = sorted(glob.glob(self.extra_fn[:-8] + 'draw_*/rage_matrices_bin.csv'))[0]\n","        else:\n","            rmatrices_file = sorted(glob.glob(self.fn[:-8] + 'draw_*/rage_matrices_bin.csv'))[0]\n","    \n","        rage_matrices = np.fromfile(rmatrices_file,dtype=np.float32).astype(self.float_type)\n","        rage_matrices = rage_matrices.reshape((4,4,4))\n","        self.VP = np.dot(np.linalg.inv(rage_matrices[0,:,:]),rage_matrices[2,:,:])\n","        self.VP_inverse = np.linalg.inv(self.VP) # multiply this matrix to convert from NDC to world coordinate\n","        self.P = np.dot(np.linalg.inv(rage_matrices[1,:,:]),rage_matrices[2,:,:])\n","        self.P_inverse = np.linalg.inv(self.P) # multiply this matrix to convert from NDC to camera coordinate\n","    def GetObjFN(self):\n","        return \".\" + self.fn[len(self.fnroot):]\n","    def GetMesh(self):\n","        return trimesh.load(self.fn)\n","    def LoadBinVox(self):\n","        with open(self.binvox, 'rb') as f:\n","            return binvox_rw.read_as_3d_array(f)\n","    def ndcs_to_pixels(self, x, y):\n","        s_y, s_x = self.IMAGE_SIZE\n","        s_x -= 1\n","        s_y -= 1\n","        xx = self.float_type(x + 1) * self.float_type(s_x / 2)\n","        yy = self.float_type(1 - y) * self.float_type(s_y / 2)\n","        return xx, yy\n","    def pixels_to_ndcs(self, xx, yy):\n","        s_y, s_x = self.IMAGE_SIZE\n","        s_x -= 1  # so 1 is being mapped into (n-1)th pixel\n","        s_y -= 1  # so 1 is being mapped into (n-1)th pixel\n","        x = self.float_type(2 / s_x) * self.float_type(xx) - 1\n","        y = self.float_type(-2 / s_y) * self.float_type(yy) + 1\n","        return x, y\n","    def ProjectObjToImage(self):\n","        obj = trimesh.load(self.fn)\n","        ndcpts = np.concatenate([obj.vertices, np.ones((obj.vertices.shape[0],1))],axis=1) @ self.P\n","        ndcpts = ndcpts[:,0:2]/ndcpts[:,-1:]\n","        xx, yy = self.ndcs_to_pixels(ndcpts[:,0], ndcpts[:,1])\n","        xxi = np.rint(xx).astype(int)\n","        yyi = np.rint(yy).astype(int)\n","        select = np.logical_and(np.logical_and(xxi>0, xxi<self.IMAGE_SIZE[1]), np.logical_and(yyi>0, yyi<self.IMAGE_SIZE[0]))\n","        return xx, yy, xxi, yyi, select, obj\n","    \n","    def __repr__(self):\n","        return \"MeshObj(\\n  {}\\n  {};  {};  {})\".format(self.fn,self.csName,self.objID,self.frameID)\n","    def __str__(self):\n","        return self.__repr__()\n","    \n","\n","    def Get_GroundTruth(self,numDepth):\n","        vis_orig = np.load(self.visfn)==self.objID\n","        gt_im = np.zeros(vis_orig.shape + (numDepth,), dtype=bool) \n","        return gt_im\n","    \n","    def getOPlanes(self,numDepth,given_depth_range=None,depth_range_expand_ratio=0.1,pure_infer=False):\n","            im = np.array(Image.open(self.imfn))\n","            depth = np.load(self.depthfn).astype(self.float_type)/6.0 - 4e-5\n","            vis_orig = np.load(self.visfn)==self.objID\n","\n","            obj = self.GetMesh()\n","\n","            py_orig, px_orig = np.nonzero(vis_orig)\n","            px = px_orig \n","            py = py_orig \n","            ndcx, ndcy = self.pixels_to_ndcs(px, py)\n","            ndcz = depth[py, px]\n","            rgb = im[py, px]\n","\n","            ndc_coord = np.stack([ndcx, ndcy, ndcz, np.ones_like(ndcz)], axis=1) # NDC\n","            camera_coord = ndc_coord @ self.P_inverse # convert to camera coordinate, [#pixels, 3]\n","            camera_coord = camera_coord[:,0:3]/camera_coord[:,-1:] # divide, [#pixels, 3]\n","\n","            # mp.plot(camera_coord, c=rgb.astype(np.double)/255, shading={\"point_size\": 0.03})\n","\n","            if given_depth_range is not None:\n","                cur_depth_range = given_depth_range\n","                if pure_infer:\n","                    # NOTE: during inference, we heavily rely on mask to give correct closest_depth value.\n","                    # Therefore, we need to be somehow conservative. \n","                    vis_for_max_Z = binary_erosion(vis_orig, np.ones((10, 10)))\n","                    tmp_py_orig, tmp_px_orig = np.nonzero(vis_for_max_Z)\n","                    tmp_px = tmp_px_orig \n","                    tmp_py = tmp_py_orig \n","                    tmp_ndcx, tmp_ndcy = self.pixels_to_ndcs(tmp_px, tmp_py)\n","                    tmp_ndcz = depth[tmp_py, tmp_px]\n","\n","                    tmp_ndc_coord = np.stack([tmp_ndcx, tmp_ndcy, tmp_ndcz, np.ones_like(tmp_ndcz)], axis=1) # NDC\n","                    tmp_camera_coord = tmp_ndc_coord @ self.P_inverse # convert to camera coordinate, [#pixels, 3]\n","                    tmp_camera_coord = tmp_camera_coord[:,0:3] / tmp_camera_coord[:,-1:] # divide, [#pixels, 3]\n","                    cur_closest_vis_depth = np.max(tmp_camera_coord[:, 2])\n","                else:\n","                    raise NotImplementedError\n","            else:\n","                cur_depth_range, cur_closest_vis_depth = self.compute_visible_mesh_depth_range()\n","                cur_depth_range = cur_depth_range * (1 + depth_range_expand_ratio)\n","\n","\n","            maxZ = np.max(camera_coord[:, 2])    # neg-Z is for forward. Therefore, maxZ is the closest depth.\n","            zVals = [random.uniform(maxZ - cur_depth_range,maxZ) for _ in range(numDepth)]\n","            aug = np.reshape(zVals, (1, numDepth)).astype(self.float_type)\n","\n","            with open(self.binvox, 'rb') as f:\n","                m1 = binvox_rw.read_as_3d_array(f)\n","\n","            coords = np.tile(camera_coord[...,np.newaxis], (1, 1, numDepth))  # [#pixels, 3, num_depth]\n","\n","            # Path along the ray\n","            coords = coords * aug[:, np.newaxis, :] / (camera_coord[:, 2:, np.newaxis] + self.avoid_nan_eps) # [#p, 3, #d], aug: [1, 1, #d], [#p, 1, 1], [#p, 1, #d]\n","\n","            coords = np.swapaxes(coords,1,2)      # [#pixels, num_depth, 3]\n","            coords = np.reshape(coords, (-1,3))   # [#pixels x num_depth, 3]\n","            # ii = np.tile(np.arange(numDepth), (coords.shape[0]//numDepth,))  # [num_depth x #pixels, ]\n","\n","            grid_coords = np.round((coords - m1.translate) / m1.scale * m1.dims - 0.5).astype(int)\n","            label = np.zeros((coords.shape[0],),dtype=bool)\n","            select = np.logical_and(np.all(grid_coords>=0,axis=1), np.all(grid_coords<m1.dims,axis=1))\n","            label[select] = m1.data[grid_coords[select,0], grid_coords[select,1], grid_coords[select,2]]\n","\n","            gt_im = np.zeros(vis_orig.shape + (numDepth,), dtype=bool)   # [new_h, new_w, num_depth]\n","            trueOPlanes = torch.zeros(numDepth,1,800,1280)\n","            for ii in range(numDepth):\n","                gt_im[py_orig, px_orig, ii] = label[ii::numDepth]\n","                trueOPlanes[ii,0] = torch.tensor([1 if x else 0 for x in gt_im[:, :, ii].reshape(800*1280)]).reshape(800,1280)\n","#                 plt.imshow(trueOPlanes[ii,0])\n","#                 plt.show()\n","            \n","\n","            return trueOPlanes, zVals\n","\n"],"metadata":{"id":"ZIrwSz-yfGV2","executionInfo":{"status":"ok","timestamp":1670184664992,"user_tz":360,"elapsed":4203,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# DATALOADER\n","import os\n","import numpy as np\n","import pandas as pd\n","from torchvision.io import read_image\n","from pathlib import Path\n","import torch\n","import os.path\n","import cv2\n","import scipy.ndimage\n","from skimage import filters\n","import random\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","class CustomImageDataset(Dataset):\n","    def __init__(self, csv_file, n_planes_for_train):\n","        # making dataframe \n","        self.df2 = pd.read_csv(csv_file) \n","        headerList = ['image', 'visible', 'depth', 'gt_oplane']\n","        self.df2.to_csv(\"inputs.csv\", header=headerList, index=False)\n","          \n","        df2 = pd.read_csv(\"inputs.csv\")\n","        #self.train = train\n","        #self.shuffle = shuffle\n","        self.n_planes_for_train = n_planes_for_train\n","        self.gt_oplanes = df2.loc[:,\"gt_oplane\"]\n","        self.img_dir = df2.loc[:,\"image\"]\n","        self.depth_dir = df2.loc[:,\"depth\"]\n","        self.visible_dir = df2.loc[:,\"visible\"]\n","        #self.transform = transform\n","        #self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_dir)\n","\n","    def grey_transform(self, np_array):\n","\n","        grey_image = cv2.cvtColor(np_array, cv2.COLOR_BGR2GRAY)\n","        return grey_image\n","\n","    def concatenate_channels(self, img1, img2, axis=2):\n","        return np.concatenate((img1,img2), axis=2)\n","      \n","    def calculate_euclidean(self, np_array):\n","        return scipy.ndimage.distance_transform_edt(np_array)\n","\n","    def calculate_edges(self, grey_img):\n","        edges = filters.farid(grey_img)\n","        return edges\n","\n","    def resize_image(self, np_array, x_dim, y_dim):\n","        image_size = (x_dim, y_dim)\n","        image_resized = cv2.resize(np_array, image_size, interpolation=cv2.INTER_LINEAR)\n","        return image_resized\n","\n","    def add_channels(self, np_array):\n","        img_resized = self.resize_image(np_array, 512, 512)\n","\n","        grey_img = self.grey_transform(img_resized)\n","        edges = self.calculate_edges(grey_img)\n","        edges_resized = self.resize_image(edges, 512, 512)\n","        edges_resized = torch.from_numpy(edges_resized).unsqueeze(-1)\n","        edges_resized = edges_resized.cpu().detach().numpy()\n","\n","        euc_dist = self.calculate_euclidean(grey_img)\n","        euc_dist_resized = torch.from_numpy(euc_dist).unsqueeze(-1)\n","        euc_dist_resized = euc_dist_resized.cpu().detach().numpy()\n","\n","        concatenated_input = self.concatenate_channels(img_resized,euc_dist_resized, axis=2)\n","        concatenated_input = self.concatenate_channels(concatenated_input,edges_resized, axis=2)\n","\n","        return concatenated_input\n","\n","    def __getitem__(self, idx):\n","        #self.__len__\n","        #if self.shuffle:\n","        #  idx = random.randint(0,self.__len__)\n","\n","        #if self.train:\n","\n","        image = cv2.imread(self.img_dir[idx])\n","        image = self.add_channels(image)\n","      \n","        label_obj = MeshObj(self.gt_oplanes[idx],max_depth_range,mesh_data_root,binvoxPathPrefix,extra_mesh_data_root)\n","        label, zVals = label_obj.getOPlanes(self.n_planes_for_train)\n","        # depth_range, max_z = label_obj.compute_visible_mesh_depth_range()\n","\n","        depth = np.load(self.depth_dir[idx])\n","        depth = self.resize_image(depth, 512,512)\n","        depth = np.expand_dims(depth, axis=0)\n","        depth = torch.from_numpy(depth)\n","\n","        visible = np.load(self.visible_dir[idx])\n","        #visible = self.resize_image(visible, 512,512)\n","        visible_low_res = self.resize_image(visible, 400, 640)\n","        visible = np.expand_dims(visible, axis=0)\n","        visible = torch.from_numpy(visible)\n","        visible_low_res = np.expand_dims(visible_low_res, axis=0)\n","        visible_low_res = torch.from_numpy(visible_low_res)\n","\n","        image = np.moveaxis(image, -1, 0)\n","        image = torch.from_numpy(image)\n","\n","        # label = np.moveaxis(label, -1, 0)\n","        # label = torch.from_numpy(label)\n","        return image, label, depth, visible, visible_low_res, zVals\n","\n"],"metadata":{"id":"iCjX-G1fIXwQ","executionInfo":{"status":"ok","timestamp":1670184667153,"user_tz":360,"elapsed":2165,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["'''FPN in PyTorch.\n","See the paper \"Feature Pyramid Networks for Object Detection\" for more details.\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import cv2\n","import time\n","from torch.autograd import Variable\n","import math\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class FPN(nn.Module):\n","    def __init__(self, block, num_blocks):\n","        super(FPN, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(5, 64, kernel_size=7, stride=2, padding=3, bias=False) #THE FIRST NUM (3) IS THE NUMBER OF INPUT CHANNELS\n","        self.bn1 = nn.BatchNorm2d(64)\n","\n","        # Bottom-up layers\n","        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","\n","        # Top layer\n","        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels\n","\n","        # Smooth layers\n","        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","\n","        # Lateral layers\n","        self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n","        self.latlayer2 = nn.Conv2d( 512, 256, kernel_size=1, stride=1, padding=0)\n","        self.latlayer3 = nn.Conv2d( 256, 256, kernel_size=1, stride=1, padding=0)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def _upsample_add(self, x, y):\n","        '''Upsample and add two feature maps.\n","        Args:\n","          x: (Variable) top feature map to be upsampled.\n","          y: (Variable) lateral feature map.\n","        Returns:\n","          (Variable) added feature map.\n","        Note in PyTorch, when input size is odd, the upsampled feature map\n","        with `F.upsample(..., scale_factor=2, mode='nearest')`\n","        maybe not equal to the lateral feature map size.\n","        e.g.\n","        original input size: [N,_,15,15] ->\n","        conv2d feature map size: [N,_,8,8] ->\n","        upsampled feature map size: [N,_,16,16]\n","        So we choose bilinear upsample which supports arbitrary output sizes.\n","        '''\n","        _,_,H,W = y.size()\n","        return F.upsample(x, size=(H,W), mode='bilinear') + y\n","\n","    def forward(self, x):\n","        # Bottom-up\n","        c1 = F.relu(self.bn1(self.conv1(x)))\n","        c1 = F.max_pool2d(c1, kernel_size=3, stride=2, padding=1)\n","        c2 = self.layer1(c1)\n","        c3 = self.layer2(c2)\n","        c4 = self.layer3(c3)\n","        c5 = self.layer4(c4)\n","        # Top-down\n","        p5 = self.toplayer(c5)\n","        p4 = self._upsample_add(p5, self.latlayer1(c4))\n","        p3 = self._upsample_add(p4, self.latlayer2(c3))\n","        p2 = self._upsample_add(p3, self.latlayer3(c2))\n","        # Smooth\n","        p4 = self.smooth1(p4)\n","        p3 = self.smooth2(p3)\n","        p2 = self.smooth3(p2)\n","        return p2\n","\n","\n","def FPN101():\n","    # return FPN(Bottleneck, [2,4,23,3])\n","    return FPN(Bottleneck, [2,2,2,2])\n","    \n","class FRGB(nn.Module):\n","    def __init__(self):\n","        super(FRGB, self).__init__()\n","        self.conv1 = nn.Conv2d(256,128,3,padding=1)\n","        self.gn1 = nn.GroupNorm(32,128)\n","        self.ReLU1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(128,128,3,padding=1)\n","        self.gn2 = nn.GroupNorm(32,128)\n","        self.ReLU2 = nn.ReLU()\n","        self.conv3 = nn.Conv2d(128,128,1)\n","        self.gn3 = nn.GroupNorm(32,128)\n","    \n","    def forward(self,x):\n","        x = self.ReLU1(self.gn1(self.conv1(x)))\n","        x = self.ReLU2(self.gn2(self.conv2(x)))\n","        x = self.gn3(self.conv3(x))\n","        return x\n","\n","class Fspatial(nn.Module):\n","    def __init__(self):\n","        super(Fspatial, self).__init__()\n","        self.conv1 = nn.Conv2d(256,128,3,padding=1)\n","        self.gn1 = nn.GroupNorm(32,128)\n","        self.ReLU1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(128,128,3,padding=1)\n","        self.gn2 = nn.GroupNorm(32,128)\n","        self.ReLU2 = nn.ReLU()\n","        self.conv3 = nn.Conv2d(128,1,1)\n","    \n","    def forward(self,x):\n","        x = self.ReLU1(self.gn1(self.conv1(x)))\n","        x = self.ReLU2(self.gn2(self.conv2(x)))\n","        x = self.conv3(x)\n","        return x\n","    \n","class TwoLayerCNN(nn.Module):\n","    def __init__(self):\n","        super(TwoLayerCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(64,128,1)\n","        self.gn1 = nn.GroupNorm(32,128)\n","        self.ReLU1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(128,128,1)\n","        self.gn2 = nn.GroupNorm(32,128)\n","    \n","    \n","    def forward(self,x):\n","        x = self.ReLU1(self.gn1(self.conv1(x)))\n","        x = self.gn2(self.conv2(x))\n","        return x\n","\n","class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, op_true, op_pred, mask, N):\n","        \n","        num = (2*mask*op_true*op_pred).sum(dim = [2,3])\n","        d1 = (mask*op_true).sum(dim=[2,3])\n","        d2 = (mask*op_pred).sum(dim=[2,3])\n","        s = (num/(d1+d2)).sum()\n","\n","        if math.isnan(s/N):\n","          print(\"Dice loss is NaN:\",s/N)\n","      \n","        return s/N\n","    \n","\n","    \n","class BCELoss(nn.Module):\n","    def __init__(self):\n","        super(BCELoss,self).__init__()\n","    \n","    def forward(self, op_true, op_pred, mask, N):\n","        num = mask*((op_true*(torch.log(op_pred+0.000001))) + (1-op_true)*torch.log(1-op_pred+0.000001))                 \n","        s = num.sum()\n","        s = -s/(N*mask.sum()) \n","\n","        if math.isnan(s):\n","          print(\"BCE loss is NaN:\",s)\n","\n","        return s\n","        \n","        \n","        \n","\n","class ComboNet(nn.Module):\n","    def __init__(self, batchSize, numPeChannels, learningRate):\n","        super(ComboNet, self).__init__()\n","        self.f_FPN = FPN(Bottleneck, [3,4,6,3])\n","        self.f_RGB = FRGB()\n","        self.f_depth = TwoLayerCNN()\n","        self.f_spatial = Fspatial()\n","        \n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=learningRate)\n","        self.lossBCE = BCELoss()\n","        self.lossDICE = DiceLoss()\n","        \n","        #Make denominator tensor for positional encoding (don't want to run duplicate work)\n","        denom = torch.zeros(batchSize,numPeChannels,256,256)\n","        idx = torch.ones(256,256)\n","        for i in range(numPeChannels):\n","            denom[:,i,:,:] = 200**(2*i*idx/numPeChannels)\n","        self.denom = denom\n","        self.numPeChannels = numPeChannels\n","\n","        self.train_mode=True\n","        self.updatedOptimizer=True\n","        \n","        \n","    def forward(self,x1,x2,z):\n","        st = time.time()\n","        #RGB feature processing\n","        x1 = self.f_FPN(x1)\n","        x1_lowres = self.f_RGB(x1)\n","        x1 = F.interpolate(x1_lowres,scale_factor=2, mode='bilinear') #Upsampling step: convert 128x128 to 256x256 img\n","        \n","        #Depth feature processing\n","        x2 = self.positionEncoding(x2,z)\n","        x2_lowres = F.interpolate(x2,scale_factor=0.5, mode='bilinear') #Downsampled depth difference image\n","        x2 = self.f_depth(x2)\n","        x2_lowres = self.f_depth(x2_lowres)\n","        \n","        #Combine features and pass them through final CNN\n","        x = torch.cat((x1,x2), 1) #second arg specifies which dimension to concatenate on, we want channel dimension which is 1\n","        x = self.f_spatial(x)\n","        \n","        #Get low res OPlane for loss computation, use inner product (eqn 14 from paper)   \n","        multp  = x1_lowres * x2_lowres \n","        x_lowres = multp.sum(dim = 1, keepdim = True)\n","        \n","        #Normalize both outputs so all values are between 0 and 1\n","        x = x - x.min()\n","        x = x/x.max()\n","        x_lowres = x_lowres - x_lowres.min()\n","        x_lowres = x_lowres/x_lowres.max()\n","        \n","        return x, x_lowres\n","    \n","    def positionEncoding(self, depth, z):\n","        \"\"\"\n","        Computes the positional encoding (as defined by the paper) for a depth\n","        - depth: the input depth image\n","        - z: the distance we wish to evaluate\n","        \"\"\"\n","        depth = F.interpolate(depth,scale_factor=0.5, mode='bilinear')\n","        s = depth.size()\n","        pe = torch.zeros(s[0],self.numPeChannels,s[2],s[3])\n","        num = z-depth\n","        pe[:,0::2,:,:] = torch.sin(50*num/self.denom[:,0::2,:])\n","        pe[:,1::2,:,:] = torch.cos(50*num/self.denom[:,1::2,:])\n","        \n","        return pe\n","    \n","    def step(self,x_RGB,x_depth,mask,z_vals,op_true_highres):\n","        \"\"\"\n","        Iterates over a single training step, ie one image with a set of N values in the range [z_min, z_max]\n","        - x: input batch\n","        - y: expected labels for batch\n","        \"\"\"\n","        self.optimizer.zero_grad() #Reset parameter gradients to 0\n","        #Get the outputs for each values of z\n","        N = len(z_vals)\n","        op_highres = torch.zeros(N,1,800,1280)\n","        op_lowres = torch.zeros(N,1,400,640)\n","        \n","        st = time.time()\n","        for i, z in enumerate(z_vals):\n","            op_highres_i, op_lowres_i = self.forward(x_RGB,x_depth,z)\n","            op_highres[i,:,:,:] = F.interpolate(op_highres_i,(800,1280), mode='bilinear')\n","            op_lowres[i,:,:,:] = F.interpolate(op_lowres_i,(400,640), mode='bilinear')\n","            \n","        \n","        #Calculate the loss based on the predicted OPlanes for all z values\n","        lambda_BCE, lambda_DICE = 1,1\n","\n","        # downsample to low resolution\n","        mask_lowres = F.interpolate(mask,scale_factor=0.5, mode='bilinear')\n","        # op_true_highres = F.interpolate(op_true_highres,scale_factor=0.5, mode='bilinear')\n","        op_true_lowres = F.interpolate(op_true_highres,scale_factor=0.5, mode='bilinear')\n","\n","        # # upscale everything to originial dimension (same as GT O-Plane)\n","        # op_true_highres = F.interpolate(op_true_highres,(800,1280), mode='bilinear')\n","        # op_highres = F.interpolate(op_highres,(800,1280), mode='bilinear')\n","        \n","        loss_highres = lambda_BCE*self.lossBCE(op_true_highres, op_highres, mask, N) + lambda_DICE*self.lossDICE(op_true_highres, op_highres, mask, N)\n","        loss_lowres = lambda_BCE*self.lossBCE(op_true_lowres, op_lowres, mask_lowres, N) + lambda_DICE*self.lossDICE(op_true_lowres, op_lowres, mask_lowres, N)    \n","        loss = loss_highres + loss_lowres\n","        loss_val = loss.item()\n","\n","        ############\n","        loss_val = loss.detach().cpu().numpy()\n","        if random.random() > 0.5:\n","          loss_val = float('nan')\n","        ############\n","\n","        if not math.isnan(loss_val) and self.train_mode:\n","          loss.backward()\n","          self.optimizer.step()\n","          self.updatedOptimizer=True\n","        else:\n","          self.updatedOptimizer=False\n","\n","        return loss_val"],"metadata":{"id":"_rZjKbWHwLFg","executionInfo":{"status":"ok","timestamp":1670184667153,"user_tz":360,"elapsed":2,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data.sampler import SubsetRandomSampler\n","import os\n","import pandas as pd\n","from torchvision.io import read_image\n","from pathlib import Path\n","import torch\n","import os.path\n","import cv2\n","import scipy.ndimage\n","import random\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","#from modules import ComboNet\n","\n","path = '/content/drive/MyDrive/ECE 544 Project/VincentsWorkspace'\n","dataset = CustomImageDataset(csv_file=path + '/' + 'inputs.csv', n_planes_for_train=5)\n","batch_size = 1\n","validation_split = .2\n","shuffle_dataset = True\n","random_seed= 42\n","\n","# Creating data indices for training and validation splits:\n","dataset_size = len(dataset)\n","\n","indices = list(range(dataset_size))\n","split = int(np.floor(validation_split * dataset_size))\n","if shuffle_dataset :\n","    np.random.seed(random_seed)\n","    np.random.shuffle(indices)\n","train_indices, val_indices = indices[split:], indices[:split]\n","\n","# Creating PT data samplers and loaders:\n","train_sampler = SubsetRandomSampler(train_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)\n","\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n","                                           sampler=train_sampler, persistent_workers=True, num_workers=4)\n","validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n","                                                sampler=valid_sampler,persistent_workers=True, num_workers=4)\n","\n","N = 10\n","net = ComboNet(1,64,0.001)\n","# Usage Example:\n","num_epochs = 3\n","totalLoss = 0\n","totalImagesProcessed = 0\n","\n","#File to write losses to\n","f = open(\"loss_training.txt\", \"w\")\n","f.write(\"Losses after each iteration during training:\")\n","f.close()\n","for epoch in range(num_epochs):\n","    print(\"Processing epoch \",epoch)\n","    f = open(\"loss_training.txt\", \"w\")\n","    f.write(\"\\n\\nEpoch \"+str(epoch))\n","    f.close()\n","\n","    # Train:   \n","    for i, data in enumerate(train_loader):\n","        \n","        st = time.time()\n","        \n","        op_truth = data[1].to(dtype=torch.float32) \n","        mask = data[3].to(dtype=torch.float32)\n","        mask_lowres = data[4].to(dtype=torch.float32)\n","        z_vals = data[5]\n","        \n","        #Get RGB and Depth images from data\n","        ipt1 = data[0].to(dtype=torch.float32) # image\n","        ipt2 = data[2].to(dtype=torch.float32) # depth\n","\n","        l = net.step(ipt1,ipt2,mask,z_vals,op_truth[0])\n","        nanLoss = math.isnan(l)\n","        if not nanLoss:\n","          totalLoss += l\n","          totalImagesProcessed += 1\n","        \n","        #Tally the loss\n","        if totalImagesProcessed == 0:\n","          avgLoss = 1\n","        else:\n","          avgLoss = round(totalLoss/totalImagesProcessed,4)\n","\n","        print(\"image \",i,int(time.time()-st), avgLoss, l, nanLoss,net.updatedOptimizer)\n","\n","        f = open(\"loss_training.txt\", \"a\")\n","        f.write(\"\\n\"+str(avgLoss))\n","        f.close()\n","\n","        #Save model\n","        if i%10 == 0:\n","          torch.save(net.state_dict(),\"comboNet.pth\")\n","        \n","\n","\n"],"metadata":{"id":"QVSVg1IAaPXJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a9bbd659-85af-4bda-fdbd-32d897c1dff4"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Processing epoch  0\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:3722: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["image  0 42 1.0946 1.0945787 False True\n","image  1 14 1.0946 nan True False\n","image  2 37 0.6526 0.21059531 False True\n","image  3 13 0.6526 nan True False\n","image  4 40 0.4724 0.11190034 False True\n","image  5 38 0.3775 0.09290096 False True\n","image  6 13 0.3775 nan True False\n","image  7 37 0.4597 0.7884029 False True\n","image  8 13 0.4597 nan True False\n","image  9 13 0.4597 nan True False\n","image  10 37 0.3976 0.087180465 False True\n","image  11 39 0.4029 0.43495595 False True\n","image  12 37 0.3799 0.21865588 False True\n","image  13 13 0.3799 nan True False\n","image  14 37 0.3693 0.2845071 False True\n","image  15 38 0.3362 0.038701385 False True\n","image  16 13 0.3362 nan True False\n","image  17 39 0.3141 0.09308059 False True\n","image  18 37 0.2934 0.06531206 False True\n","image  19 13 0.2934 nan True False\n","image  20 13 0.2934 nan True False\n","image  21 38 0.2975 0.3472028 False True\n","image  22 37 0.2795 0.04559303 False True\n","image  23 13 0.2795 nan True False\n","image  24 13 0.2795 nan True False\n","image  25 13 0.2795 nan True False\n","image  26 38 0.2866 0.38544375 False True\n","image  27 37 0.274 0.08542752 False True\n","image  28 37 0.2715 0.23135294 False True\n","image  29 13 0.2715 nan True False\n","image  30 37 0.2594 0.05383175 False True\n","image  31 13 0.2594 nan True False\n","image  32 38 0.259 0.25108585 False True\n","image  33 13 0.259 nan True False\n","image  34 36 0.2553 0.18431213 False True\n","image  35 37 0.2477 0.09704322 False True\n","image  36 13 0.2477 nan True False\n","image  37 13 0.2477 nan True False\n","image  38 36 0.2828 1.019031 False True\n","image  39 13 0.2828 nan True False\n","image  40 13 0.2828 nan True False\n","image  41 38 0.2733 0.06541912 False True\n","image  42 13 0.2733 nan True False\n","image  43 13 0.2733 nan True False\n","image  44 13 0.2733 nan True False\n","image  45 13 0.2733 nan True False\n","image  46 13 0.2733 nan True False\n","image  47 38 0.2718 0.2366944 False True\n","image  48 13 0.2718 nan True False\n","image  49 36 0.2656 0.116382584 False True\n","image  50 38 0.2721 0.43628904 False True\n","image  51 13 0.2721 nan True False\n","image  52 37 0.2753 0.35642204 False True\n","image  53 38 0.3393 2.067011 False True\n","image  54 38 0.3299 0.06747082 False True\n","image  55 38 0.3252 0.18783773 False True\n","image  56 13 0.3252 nan True False\n","image  57 13 0.3252 nan True False\n","image  58 13 0.3252 nan True False\n","image  59 13 0.3252 nan True False\n","image  60 13 0.3252 nan True False\n","image  61 13 0.3252 nan True False\n"]}]},{"cell_type":"code","source":["#Load the saved net from the training\n","net = ComboNet(1,64,0.001)\n","net.load_state_dict(torch.load(\"comboNet.pth\"))\n","net.train_mode = False\n","\n","validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n","                                                sampler=valid_sampler,persistent_workers=True, num_workers=4)\n","\n","\n","#File to write losses to\n","f = open(\"loss_validation.txt\", \"w\")\n","f.write(\"Losses after each iteration with the validation set:\")\n","f.close()\n","\n","\n","for i, data in enumerate(validation_loader):\n","    st = time.time()\n","        \n","    op_truth = data[1].to(dtype=torch.float32) \n","    mask = data[3].to(dtype=torch.float32)\n","    mask_lowres = data[4].to(dtype=torch.float32)\n","    z_vals = data[5]\n","    \n","    #Get RGB and Depth images from data\n","    ipt1 = data[0].to(dtype=torch.float32) # image\n","    ipt2 = data[2].to(dtype=torch.float32) # depth\n","\n","    l = net.step(ipt1,ipt2,mask,z_vals,op_truth[0])\n","\n","    #Tally the loss\n","    if not math.isnan(l):\n","      totalLoss += l\n","      totalImagesProcessed += 1\n","    \n","    loss = round(l.item(),4)\n","    print(\"image \",i,loss)\n","\n","    f = open(\"loss_validation.txt\", \"a\")\n","    f.write(\"\\n\"+str(loss))\n","    f.close()\n","  \n","\n","#Add total average loss at the end\n","f = open(\"loss_validation.txt\", \"a\")\n","f.write(\"\\nTotal average loss over all validation set: \"+str(round(totalLoss/totalImagesProcessed,4)))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":570},"id":"FwKqs5KPz5Ji","executionInfo":{"status":"error","timestamp":1670119514896,"user_tz":360,"elapsed":112294,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}},"outputId":"d65d5e01-4ac3-491b-ab01-a719163bba57"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:3722: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"]},{"output_type":"stream","name":"stdout","text":["image  0 0.2111\n","1\n","image  1 1.2685\n","2\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-b486347eb767>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mipt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mipt2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mop_truth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#Tally the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-ff931dc092f9>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, x_RGB, x_depth, mask, z_vals, op_true_highres)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0mop_highres_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_lowres_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_RGB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m             \u001b[0mop_highres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_highres_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1280\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mop_lowres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_lowres_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-ff931dc092f9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2, z)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m#Combine features and pass them through final CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#second arg specifies which dimension to concatenate on, we want channel dimension which is 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_spatial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m#Get low res OPlane for loss computation, use inner product (eqn 14 from paper)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-ff931dc092f9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    451\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 453\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    454\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}